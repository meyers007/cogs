{% extends "common.html" %}
{% block content %}

<style>
.recording-btn {
    background-color: #dc3545;
    border-color: #dc3545;
    color: white;
}
.recording-btn.active {
    background-color: #28a745;
    border-color: #28a745;
    animation: pulse 1.5s infinite;
}
@keyframes pulse {
    0% { box-shadow: 0 0 0 0 rgba(40, 167, 69, 0.7); }
    70% { box-shadow: 0 0 0 10px rgba(40, 167, 69, 0); }
    100% { box-shadow: 0 0 0 0 rgba(40, 167, 69, 0); }
}
.transcription-output {
    border: 1px solid #ddd;
    padding: 15px;
    min-height: 200px;
    background-color: #f8f9fa;
    border-radius: 5px;
    margin-top: 10px;
}
</style>

<div class="container mt-4">
    <h2>Audio Transcription</h2>
    
    <div class="row">
        <div class="col-md-12">
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">Recording Controls</h5>
                    
                    <button id="toggleRecordingBtn" class="btn recording-btn" onclick="toggleTranscription()">
                        <i class="fa fa-microphone"></i> Start Recording
                    </button>
                    
                    <button id="clearBtn" class="btn btn-secondary ml-2" onclick="clearTranscription()">
                        <i class="fa fa-trash"></i> Clear
                    </button>
                    
                    <div class="mt-3">
                        <small class="text-muted">Status: <span id="statusText">Ready</span></small>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <div class="row mt-3">
        <div class="col-md-12">
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">Transcription Results</h5>
                    <div id="transcriptionOutput" class="transcription-output">
                        <em>Transcription results will appear here...</em>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
let mediaRecorder;
let audioChunks = [];
let isRecording = false;
let isTranscribing = false;
let transcriptionResults = [];
let transcriptionResultsText = "";
let audioStream;

// Whisper API configuration
//const WHISPER_API_URL = 'http://localhost:8000/v1/audio/transcriptions'; // Adjust URL as needed
const WHISPER_API_URL = 'http://localhost:8000//geoaudio/transcribe/'; // Adjust URL as needed
const CHUNK_DURATION = 2000; // 5 seconds chunks
let recordingInterval;

// Initialize audio recording
async function initializeAudioRecording() {
    try {
        audioStream = await navigator.mediaDevices.getUserMedia({ 
            audio: {
                sampleRate: 16000,
                channelCount: 1,
                echoCancellation: true,
                noiseSuppression: true
            } 
        });
        
        // Try different audio formats, preferring ones that work well with WAV conversion
        let options = [
            { mimeType: 'audio/webm;codecs=opus' },
            { mimeType: 'audio/webm' },
            { mimeType: 'audio/ogg;codecs=opus' },
            { mimeType: 'audio/mp4' }
        ];
        
        let selectedOptions = null;
        for (let option of options) {
            if (MediaRecorder.isTypeSupported(option.mimeType)) {
                selectedOptions = option;
                break;
            }
        }
        
        mediaRecorder = new MediaRecorder(audioStream, selectedOptions || {});
        
        mediaRecorder.ondataavailable = function(event) {
            //console.log("On Data: ", event.data.size)
            if (event.data.size > 0) {
                audioChunks.push(event.data);
            }
        };
        
        mediaRecorder.onstop = function() {
            if (audioChunks.length > 0) {
                processAudioChunk();
            }
        };
        
        return true;
    } catch (error) {
        console.error('Error accessing microphone:', error);
        onTranscriptionError('Microphone access denied');
        return false;
    }
}

// Process audio chunk and send to Whisper
async function processAudioChunk() {
    if (audioChunks.length === 0) return;
    
    try {
        const audioBlob = new Blob(audioChunks, { type: mediaRecorder.mimeType || 'audio/webm' });
        audioChunks = []; // Clear chunks
        
        // Convert to WAV format
        const wavBlob = await convertToWAV(audioBlob);
        
        // Send WAV file to Whisper API
        const formData = new FormData();
        formData.append('file', wavBlob, 'audio.wav');
        formData.append('model', 'whisper-1');
        formData.append('language', 'en');
        formData.append('response_format', 'json');
        
        const response = await fetch(WHISPER_API_URL, {
            method: 'POST',
            body: formData
        });
        
        if (response.ok) {
            const result = await response.text();
            if (result && result.trim()) {
                onTranscriptionResult(result.trim(), '');
            }
        } else {
            throw new Error(`Whisper API error: ${response.status}`);
        }
        
    } catch (error) {
        console.error('Error processing audio chunk:', error);
        onTranscriptionError(error.message);
    }
}

// Start continuous transcription
async function startTranscription() {
    if (isRecording) return;
    
    if (!mediaRecorder) {
        const initialized = await initializeAudioRecording();
        if (!initialized) {
            alert('Could not access microphone for transcription');
            return;
        }
    }
    
    try {
        isRecording = true;
        isTranscribing = true;
        onTranscriptionStart();
        
        // Start recording in chunks
        startChunkedRecording();
        
    } catch (error) {
        console.error('Error starting transcription:', error);
        onTranscriptionError(error.message);
        isRecording = false;
        isTranscribing = false;
    }
}

// Start chunked recording for continuous transcription
function startChunkedRecording() {
    if (!mediaRecorder || !isRecording) return;
    
    // Start recording
    audioChunks = [];
    mediaRecorder.start();
    
    // Set interval to stop and restart recording every CHUNK_DURATION
    recordingInterval = setInterval(() => {
        if (isRecording && mediaRecorder.state === 'recording') {
            mediaRecorder.stop();
            
            // Restart recording after a brief pause
            setTimeout(() => {
                if (isRecording) {
                    audioChunks = [];
                    mediaRecorder.start();
                }
            }, 50);
        }
    }, CHUNK_DURATION);
}

// Stop transcription
function stopTranscription() {
    if (!isRecording) return;
    
    isRecording = false;
    isTranscribing = false;
    
    if (recordingInterval) {
        clearInterval(recordingInterval);
        recordingInterval = null;
    }
    
    if (mediaRecorder && mediaRecorder.state === 'recording') {
        mediaRecorder.stop();
    }
    
    onTranscriptionEnd();
}

// Toggle transcription
function toggleTranscription() {
    if (isRecording) {
        stopTranscription();
    } else {
        startTranscription();
    }
}

// Transcribe a single audio file
async function transcribeAudioFile(audioFile) {
    try {
        const formData = new FormData();
        formData.append('file', audioFile);
        formData.append('model', 'whisper-1');
        formData.append('language', 'en');
        formData.append('response_format', 'json');
        
        const response = await fetch(WHISPER_API_URL, {
            method: 'POST',
            body: formData
        });
        
        if (response.ok) {
            const result = await response.json();
            return result.text || '';
        } else {
            throw new Error(`Whisper API error: ${response.status}`);
        }
        
    } catch (error) {
        console.error('Error transcribing audio file:', error);
        throw error;
    }
}

// Clear transcription results
function clearTranscription() {
    transcriptionResults = [];
    transcriptionResultsText = ""
    onTranscriptionClear();
}

// Set Whisper API URL
function setWhisperApiUrl(url) {
    WHISPER_API_URL = url;
}

// Event callbacks - Updated to interact with UI
function onTranscriptionStart() {
    console.log('Whisper transcription started');
    const btn = document.getElementById('toggleRecordingBtn');
    const status = document.getElementById('statusText');
    
    btn.innerHTML = '<i class="fa fa-stop"></i> Stop Recording';
    btn.classList.add('active');
    status.textContent = 'Recording...';
}

function onTranscriptionResult(finalText, interimText) {
    console.log('Transcription result:', finalText);
    
    if (finalText) {
        transcriptionResults.push({
            text: finalText,
            timestamp: new Date().toISOString()
        });
        transcriptionResultsText += finalText + " "
        // Update UI with new transcription
        updateTranscriptionDisplay();
    }
}

function onTranscriptionError(error) {
    console.error('Transcription error:', error);
    isRecording = false;
    isTranscribing = false;
    
    const btn = document.getElementById('toggleRecordingBtn');
    const status = document.getElementById('statusText');
    
    btn.innerHTML = '<i class="fa fa-microphone"></i> Start Recording';
    btn.classList.remove('active');
    status.textContent = 'Error: ' + error;
    
    alert('Transcription error: ' + error);
}

function onTranscriptionEnd() {
    console.log('Whisper transcription ended');
    const btn = document.getElementById('toggleRecordingBtn');
    const status = document.getElementById('statusText');
    
    btn.innerHTML = '<i class="fa fa-microphone"></i> Start Recording';
    btn.classList.remove('active');
    status.textContent = 'Ready';
}

function onTranscriptionClear() {
    console.log('Transcription cleared');
    const output = document.getElementById('transcriptionOutput');
    output.innerHTML = '<em>Transcription results will appear here...</em>';
}

// Get all transcription results
function getTranscriptionResults() {
    return transcriptionResults;
}

// Get transcription status
function getTranscriptionStatus() {
    return {
        isRecording: isRecording,
        isTranscribing: isTranscribing,
        isSupported: 'mediaDevices' in navigator && 'getUserMedia' in navigator.mediaDevices,
        resultsCount: transcriptionResults.length
    };
}

// Convert audio blob to WAV format
async function convertToWAV(audioBlob) {
    try {
        // Create audio context for processing
        const audioContext = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: 16000 // Match the recording sample rate
        });
        
        // Convert blob to array buffer
        const arrayBuffer = await audioBlob.arrayBuffer();
        
        // Decode audio data
        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
        
        // Convert to WAV
        const wavBuffer = audioBufferToWav(audioBuffer);
        
        // Create WAV blob
        const wavBlob = new Blob([wavBuffer], { type: 'audio/wav' });
        
        // Close audio context to free resources
        audioContext.close();
        
        return wavBlob;
        
    } catch (error) {
        console.error('Error converting to WAV:', error);
        // Fallback: return original blob with WAV mime type
        return new Blob([audioBlob], { type: 'audio/wav' });
    }
}

// Convert AudioBuffer to WAV format
function audioBufferToWav(buffer) {
    const length = buffer.length;
    const numberOfChannels = buffer.numberOfChannels;
    const sampleRate = buffer.sampleRate;
    const bitsPerSample = 16;
    const bytesPerSample = bitsPerSample / 8;
    const blockAlign = numberOfChannels * bytesPerSample;
    const byteRate = sampleRate * blockAlign;
    const dataSize = length * blockAlign;
    const bufferSize = 44 + dataSize;
    
    const arrayBuffer = new ArrayBuffer(bufferSize);
    const view = new DataView(arrayBuffer);
    
    // WAV header
    const writeString = (offset, string) => {
        for (let i = 0; i < string.length; i++) {
            view.setUint8(offset + i, string.charCodeAt(i));
        }
    };
    
    writeString(0, 'RIFF');
    view.setUint32(4, bufferSize - 8, true);
    writeString(8, 'WAVE');
    writeString(12, 'fmt ');
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true);
    view.setUint16(22, numberOfChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, byteRate, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, bitsPerSample, true);
    writeString(36, 'data');
    view.setUint32(40, dataSize, true);
    
    // Convert audio data
    let offset = 44;
    for (let i = 0; i < length; i++) {
        for (let channel = 0; channel < numberOfChannels; channel++) {
            const sample = Math.max(-1, Math.min(1, buffer.getChannelData(channel)[i]));
            const intSample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
            view.setInt16(offset, intSample, true);
            offset += 2;
        }
    }
    
    return arrayBuffer;
}

// Update the transcription display with current results
function updateTranscriptionDisplay() {
    const output = document.getElementById('transcriptionOutput');
    
    if (transcriptionResults.length === 0) {
        output.innerHTML = '<em>Transcription results will appear here...</em>';
        return;
    }
    
    let html = '';
    transcriptionResults.forEach((result, index) => {
        const time = new Date(result.timestamp).toLocaleTimeString();
        html += `<div class="transcription-item mb-2">
            <small class="text-muted">[${time}]</small> 
            <span class="transcription-text">${result.text}</span>
        </div>`;
    });
    
    output.innerHTML = html;
    
    // Auto-scroll to bottom
    output.scrollTop = output.scrollHeight;
}
// Update the transcription display with current results
function updateTranscriptionDisplay() {
    const output = document.getElementById('transcriptionOutput');
    
    if (transcriptionResults.length === 0) {
        output.innerHTML = '<em>Transcription results will appear here...</em>';
        return;
    }
    output.innerHTML = transcriptionResultsText;
    output.scrollTop = output.scrollHeight;
}


// Cleanup function
function cleanup() {
    stopTranscription();
    
    if (audioStream) {
        audioStream.getTracks().forEach(track => track.stop());
        audioStream = null;
    }
    
    if (recordingInterval) {
        clearInterval(recordingInterval);
        recordingInterval = null;
    }
}

// Initialize on page load
document.addEventListener('DOMContentLoaded', function() {
    // Auto-initialize is not needed for Whisper backend
    console.log('Whisper transcription ready');
});

// Cleanup on page unload
window.addEventListener('beforeunload', cleanup);
</script>

{% endblock %}
